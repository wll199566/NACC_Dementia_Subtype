{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear regression with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../preprocess_data/processed_data/processed_pkl/first_visit_features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"train_features.pkl\", \"rb\") as fin:\n",
    "    train_features = pickle.load(fin)\n",
    "\n",
    "with open(data_path + \"valid_features.pkl\", \"rb\") as fin:\n",
    "    valid_features = pickle.load(fin)\n",
    "    \n",
    "with open(data_path + \"test_features.pkl\", \"rb\") as fin:\n",
    "    test_features = pickle.load(fin)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1127, 380, 380)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features), len(valid_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_features[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_list = [feature[0] for feature in train_features]\n",
    "train_labels = np.array([feature[1] for feature in train_features])\n",
    "train_features_stacked = np.stack( train_features_list, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_stacked#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 2, 2, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features_list = [feature[0] for feature in valid_features]\n",
    "valid_labels = np.array([feature[1] for feature in valid_features])\n",
    "valid_features_stacked = np.stack( valid_features_list, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_list = [feature[0] for feature in test_features]\n",
    "test_labels = np.array([feature[1] for feature in test_features])\n",
    "test_features_stacked = np.stack( test_features_list, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((380, 557), (380, 557))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_features_stacked.shape, test_features_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " random_state = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the function to get the result ##\n",
    "def result_statistics(y_pred, y_true):\n",
    "    \n",
    "    mtx = confusion_matrix(test_labels, test_pred)\n",
    "    f1_macro = f1_score(test_labels, test_pred, average=\"macro\")\n",
    "    f1_micro = f1_score(test_labels, test_pred, average=\"micro\")\n",
    "    f1 = f1_score(test_labels, test_pred, average=None)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(mtx)\n",
    "    print(\"\\nF1 scores:\")\n",
    "    print(f1_macro, f1_micro, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"none\", class_weight='balanced', random_state=random_state, solver='saga', multi_class='multinomial', max_iter=10000).fit(train_features_stacked, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[69  4 50 29]\n",
      " [ 2  0  2  6]\n",
      " [46  3 39 27]\n",
      " [24  3 17 59]]\n",
      "\n",
      "F1 scores:\n",
      "0.33688781503280696 0.4394736842105263 [0.47098976 0.         0.34977578 0.52678571]\n"
     ]
    }
   ],
   "source": [
    "valid_pred = clf.predict(valid_features_stacked)\n",
    "result_statistics(valid_labels, valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[69  4 50 29]\n",
      " [ 2  0  2  6]\n",
      " [46  3 39 27]\n",
      " [24  3 17 59]]\n",
      "\n",
      "F1 scores:\n",
      "0.33688781503280696 0.4394736842105263 [0.47098976 0.         0.34977578 0.52678571]\n"
     ]
    }
   ],
   "source": [
    "test_pred = clf.predict(test_features_stacked)\n",
    "result_statistics(test_labels, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6e5bd148bbcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: score() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "def LR_crossvalidation(Cs):\n",
    "    # train all LR model with c in Cs\n",
    "    for c in Cs:\n",
    "        clf = LogisticRegression(penalty=\"l1\", C=c, class_weight='balanced', random_state=random_state, solver='saga', multi_class='multinomial', max_iter=10000).fit(train_features_stacked, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
