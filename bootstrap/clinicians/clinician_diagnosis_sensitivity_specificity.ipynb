{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get clinician diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/share/apps/python/gpu/3.6.5/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../preprocess_data/processed_data/processed_csv/first_visit_features/test_first_visit_features.csv\", \"rt\") as fin:\n",
    "    test_features = pd.read_csv(fin, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clinical diagonsis\n",
    "def label_clinical_diagonsis(row):\n",
    "    if (row[\"NACCALZD\"] == 1) and (row[\"NACCLBDE\"] != 1):\n",
    "        return 0\n",
    "    elif (row[\"NACCALZD\"] != 1) and (row[\"NACCLBDE\"] == 1):\n",
    "        return 1\n",
    "    elif (row[\"NACCALZD\"] == 1) and (row[\"NACCLBDE\"] == 1):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add a column and fill clinical diagnosis for each row in test set\n",
    "test_features[\"clinician_diagnosis\"] = test_features.apply(lambda row: label_clinical_diagonsis(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 380)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_features), len(pd.unique(test_features[\"NACCID\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the bootstrapping samples and get the sensitivity and specificity for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: get a list contraining (naccid, label, clinician_diagnosis)\n",
    "test_label_diag = test_features[[\"NACCID\", \"label\", \"clinician_diagnosis\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['NACC014159', 0, 0],\n",
       "       ['NACC017967', 0, 0],\n",
       "       ['NACC020729', 3, 2],\n",
       "       ...,\n",
       "       ['NACC988182', 1, 1],\n",
       "       ['NACC993747', 0, 0],\n",
       "       ['NACC997719', 3, 3]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: define a function to get the bootstrap sample sets\n",
    "def construct_bootstrap_test_samples(feature_list, sample_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - feature_list: the list containing all test features\n",
    "        - sample_size: the number of samples in each bootstrap sample\n",
    "    \"\"\"\n",
    "    index_array = np.arange(len(feature_list))  # get the index array for test_features\n",
    "    resampled_index_array = resample(index_array, n_samples=sample_size)\n",
    "    #resampled_index_list = resampled_index_array.tolist()\n",
    "    features_resampled = [feature_list[i] for i in resampled_index_array]\n",
    "\n",
    "    return features_resampled, resampled_index_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: use sklearn to compute the confusion matrix \n",
    "# and use bootstrapping_for_sensitivity_specificity to \n",
    "# compute sensitivity and specificity for each bootstrapping sample\n",
    "\n",
    "from sensitivity_specificity import sensitivity_specificity_from_mtx  \n",
    "\n",
    "def compute_sensitivity_specificity(sampled_array):\n",
    "    \"\"\"\n",
    "    Compute sensitivity and specificity scores for each disease\n",
    "    Args:\n",
    "        - sampled_array: numpy array containing resampled test sample.\n",
    "                         each sample has the format array([naccid, label, clinician_diagnosis])\n",
    "    \"\"\"\n",
    "    # get labels and the corresponding clincian diagnosis\n",
    "    resampled_labels = [sample[1] for sample in sampled_array]\n",
    "    resampled_clinician_diag = [sample[2] for sample in sampled_array]\n",
    "    \n",
    "    conf_mtx = confusion_matrix(resampled_labels, resampled_clinician_diag)\n",
    "    sensitivity, specificity = sensitivity_specificity_from_mtx(conf_mtx)\n",
    "    \n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: use bootstrap to get sensitivity and specificity for each bootstrap sample set\n",
    "\n",
    "# configure bootstrap\n",
    "n_iterations = 1000\n",
    "n_size = int(len(test_label_diag)*0.8)\n",
    "\n",
    "# define lists to store statistics\n",
    "ad_sensitivity_list = []  \n",
    "lbd_sensitivity_list = [] \n",
    "mix_sensitivity_list = [] \n",
    "others_sensitivity_list = [] \n",
    "\n",
    "ad_specificity_list = []  \n",
    "lbd_specificity_list = [] \n",
    "mix_specificity_list = [] \n",
    "others_specificity_list = [] \n",
    "\n",
    "sample_indices_list = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "\n",
    "    # prepare bootstrap test set\n",
    "    test_set, sample_indices = construct_bootstrap_test_samples(test_label_diag, n_size)\n",
    "    sample_indices_list.append(sample_indices)\n",
    "\n",
    "    # compute the sensitivity and specificity\n",
    "    sensitivity_list, specificity_list = compute_sensitivity_specificity(test_set)\n",
    "    ad_sensitivity_list.append(sensitivity_list[0])  \n",
    "    lbd_sensitivity_list.append(sensitivity_list[1])\n",
    "    mix_sensitivity_list.append(sensitivity_list[2]) \n",
    "    others_sensitivity_list.append(sensitivity_list[3])\n",
    "\n",
    "    ad_specificity_list.append(specificity_list[0])\n",
    "    lbd_specificity_list.append(specificity_list[1])\n",
    "    mix_specificity_list.append(specificity_list[2])\n",
    "    others_specificity_list.append(specificity_list[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ad_specificity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the bootstrap statistics for each metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the confident interval for the metrics\",\n",
    "def get_confident_interval(metrics_list, alpha):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - metrics_list: list containing all the bootstrap value for this metrics\",\n",
    "        - alpha: the chosen percentile\n",
    "    \"\"\"\n",
    "    \n",
    "    p = ((1.0 - alpha) / 2.0) * 100,\n",
    "    lower = np.percentile(metrics_list, p),\n",
    "    p = (alpha + ((1.0-alpha)/2.0)) * 100,\n",
    "    upper = np.percentile(metrics_list, p),\n",
    "    print(\"{}% confidence interval {} and {}\".format(alpha*100, lower[0], upper[0]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PURE AD\n",
      "mean is 0.6799437264825531\n",
      "95.0% confidence interval [0.59677081] and [0.76106195]\n",
      "\n",
      "\n",
      "PURE LBD\n",
      "mean is 0.40548237391693276\n",
      "95.0% confidence interval [0.] and [0.8]\n",
      "\n",
      "\n",
      "MIX AD + LBD\n",
      "mean is 0.03323714693598768\n",
      "95.0% confidence interval [0.] and [0.07528927]\n",
      "\n",
      "\n",
      "OTHERS\n",
      "mean is 0.6709451781330792\n",
      "95.0% confidence interval [0.56663043] and [0.76394751]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######### Sensitivity ############\n",
    "\n",
    "# AD \n",
    "print(\"PURE AD\")\n",
    "print(\"mean is\", mean(ad_sensitivity_list))\n",
    "get_confident_interval(ad_sensitivity_list, alpha)\n",
    "\n",
    "# LBD \n",
    "print(\"PURE LBD\")\n",
    "print(\"mean is\", mean(lbd_sensitivity_list))\n",
    "get_confident_interval(lbd_sensitivity_list, alpha)\n",
    "# MIX\n",
    "print(\"MIX AD + LBD\")\n",
    "print(\"mean is\", mean(mix_sensitivity_list))\n",
    "get_confident_interval(mix_sensitivity_list, alpha)\n",
    "# Others\n",
    "print(\"OTHERS\")\n",
    "print(\"mean is\", mean(others_sensitivity_list))\n",
    "get_confident_interval(others_sensitivity_list, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PURE AD\n",
      "mean is 0.495211214687952\n",
      "95.0% confidence interval [0.42549144] and [0.56253324]\n",
      "\n",
      "\n",
      "PURE LBD\n",
      "mean is 0.964865379945087\n",
      "95.0% confidence interval [0.94217687] and [0.98327759]\n",
      "\n",
      "\n",
      "MIX AD + LBD\n",
      "mean is 0.9700294942668849\n",
      "95.0% confidence interval [0.94633158] and [0.99010263]\n",
      "\n",
      "\n",
      "OTHERS\n",
      "mean is 0.7702858308652755\n",
      "95.0% confidence interval [0.71425368] and [0.82193061]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######### Specificity ############\n",
    "\n",
    "# AD \n",
    "print(\"PURE AD\")\n",
    "print(\"mean is\", mean(ad_specificity_list))\n",
    "get_confident_interval(ad_specificity_list, alpha)\n",
    "\n",
    "# LBD \n",
    "print(\"PURE LBD\")\n",
    "print(\"mean is\", mean(lbd_specificity_list))\n",
    "get_confident_interval(lbd_specificity_list, alpha)\n",
    "# MIX\n",
    "print(\"MIX AD + LBD\")\n",
    "print(\"mean is\", mean(mix_specificity_list))\n",
    "get_confident_interval(mix_specificity_list, alpha)\n",
    "# Others\n",
    "print(\"OTHERS\")\n",
    "print(\"mean is\", mean(others_specificity_list))\n",
    "get_confident_interval(others_specificity_list, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
